{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d176ce",
   "metadata": {},
   "source": [
    "# Assignment 2 (Logical AND Gate and Initial Parameters):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419c4bf",
   "metadata": {},
   "source": [
    "- Useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"returns the sigmoid function value for the input x.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of real numbers\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: sigmoid function value for each element in x\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\"returns the derivative of the sigmoid function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): numpy array of real numbers\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: derivative of the sigmoid function for each element in x\n",
    "    \"\"\"\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def update_sgd(weights, x, y, learning_rate):\n",
    "    \"\"\"Performs a single epoch of stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        weights (numpy.ndarray): current weights\n",
    "        x (numpy.ndarray): input data\n",
    "        y (numpy.ndarray): target values\n",
    "        learning_rate (float): learning rate for the update\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: updated weights\n",
    "    \"\"\"\n",
    "    new_weights = np.copy(weights) # Create a copy of weights to avoid modifying the original\n",
    "    for k in range(len(x)):        # Iterate over each sample\n",
    "        x_k = x[k]                 # Get the k-th input sample\n",
    "        y_k = y[k]                 # Get the k-th target value\n",
    "        z = np.dot(weights, x_k)   # Compute the linear combination\n",
    "        error = y_k - sigmoid(z)   # Calculate the error\n",
    "        gradient = dsigmoid(z) * error  # Compute the gradient\n",
    "        new_weights += learning_rate * gradient * x_k  # Update the weights\n",
    "    return new_weights\n",
    "\n",
    "def update_batched_gd(weights, x, y, learning_rate):\n",
    "    \"\"\"Performs a single epoch of batched gradient descent.\n",
    "\n",
    "    Args:\n",
    "        weights (numpy.ndarray): current weights\n",
    "        x (numpy.ndarray): input data\n",
    "        y (numpy.ndarray): target values\n",
    "        learning_rate (float): learning rate for the update\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: updated weights\n",
    "    \"\"\"\n",
    "    z = np.dot(x, weights)            # Compute the linear combination for all samples\n",
    "    errors = y - sigmoid(z)           # Calculate the errors for all samples\n",
    "    gradients = dsigmoid(z) * errors  # Compute the gradients for all samples\n",
    "    new_weights = weights + learning_rate * np.dot(x.T, gradients) / x.shape[0]  # Update the weights\n",
    "    return new_weights\n",
    "\n",
    "def train_sgd(weights, x, y, epochs, learning_rate):\n",
    "    \"\"\"Trains the model using stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        weights (numpy.ndarray): initial weights\n",
    "        x (numpy.ndarray): input data\n",
    "        y (numpy.ndarray): target values\n",
    "        epochs (int): number of epochs to train\n",
    "        learning_rate (float): learning rate for the update\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: final weights after training\n",
    "        numpy.ndarray: error history\n",
    "    \"\"\"\n",
    "    trained_weights = np.copy(weights)  # Create a copy of weights to avoid modifying the original\n",
    "    error_history = np.zeros(epochs)    # Initialize error history\n",
    "    for epoch in range(epochs):\n",
    "        trained_weights = update_sgd(trained_weights, x, y, learning_rate)  # Update weights\n",
    "        z = np.dot(x, trained_weights)  # Compute the linear combination\n",
    "        error = y - sigmoid(z)          # Calculate the error\n",
    "        error_history[epoch] = np.mean(error ** 2)  # Store the mean square error\n",
    "        \n",
    "    return trained_weights, error_history  # Return final weights and error history\n",
    "\n",
    "def train_batched_gd(weights, x, y, epochs, learning_rate):\n",
    "    \"\"\"Trains the model using batched gradient descent.\n",
    "\n",
    "    Args:\n",
    "        weights (numpy.ndarray): initial weights\n",
    "        x (numpy.ndarray): input data\n",
    "        y (numpy.ndarray): target values\n",
    "        epochs (int): number of epochs to train\n",
    "        learning_rate (float): learning rate for the update\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: final weights after training\n",
    "        numpy.ndarray: error history\n",
    "    \"\"\"\n",
    "    trained_weights = np.copy(weights)  # Create a copy of weights to avoid modifying the original\n",
    "    error_history = np.zeros(epochs)    # Initialize error history\n",
    "    for epoch in range(epochs):\n",
    "        trained_weights = update_batched_gd(trained_weights, x, y, learning_rate)  # Update weights\n",
    "        z = np.dot(x, trained_weights)  # Compute the linear combination\n",
    "        error = y - sigmoid(z)          # Calculate the error\n",
    "        error_history[epoch] = np.mean(error ** 2)  # Store the mean square error\n",
    "        \n",
    "    return trained_weights, error_history  # Return final weights and error history\n",
    "\n",
    "def inference(weights, x):\n",
    "    \"\"\"Performs inference using the trained weights.\n",
    "\n",
    "    Args:\n",
    "        weights (numpy.ndarray): trained weights\n",
    "        x (numpy.ndarray): input data\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: predicted values\n",
    "    \"\"\"\n",
    "    z = np.dot(x, weights)  # Compute the linear combination\n",
    "    return sigmoid(z)       # Return the sigmoid activation of the linear combination\n",
    "\n",
    "def plot_error_historys(error_history_sgd, error_history_gd):\n",
    "    \"\"\"Plots the error history of two training methods.\n",
    "\n",
    "    Args:\n",
    "        error_history_sgd (numpy.ndarray): error history for the first method\n",
    "        error_history_gd (numpy.ndarray): error history for the second method\n",
    "    \"\"\"\n",
    "    plt.plot(error_history_sgd, label='Stochastic Gradient Descent')\n",
    "    plt.plot(error_history_gd, label='Batched Gradient Descent')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Error History Comparison')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_surface(func, x_range, y_range, title='Surface plot of the function'):\n",
    "    \"\"\"Plots the surface of a function over a specified range.\n",
    "\n",
    "    Args:\n",
    "        func (Callable[[float, float], float]): function to plot f:R^2 -> R\n",
    "        x_range (tuple): range for x-axis (min, max)\n",
    "        y_range (tuple): range for y-axis (min, max)\n",
    "        title (str): title of the plot\n",
    "    \"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 100)\n",
    "    y = np.linspace(y_range[0], y_range[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.vectorize(func)(X, Y)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis')\n",
    "\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('function value')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83008bcf",
   "metadata": {},
   "source": [
    "- Data for AND gate problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gate inputs\n",
    "# last column is bias term\n",
    "X = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "Y = np.array([0, 0, 0, 1])  # AND gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8c09b",
   "metadata": {},
   "source": [
    "2-1) [10 pts] Run the given code without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weight = np.array([3332322.1, -323256.6, 772213.0])\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "trained_weights_sgd, error_history_sgd = train_sgd(initial_weight, X, Y, epochs, learning_rate)\n",
    "trained_weights_gd, error_history_gd = train_batched_gd(initial_weight, X, Y, epochs, learning_rate)\n",
    "\n",
    "# inference\n",
    "for x, y in zip(X, Y):\n",
    "    prediction_sgd = inference(trained_weights_sgd, x)\n",
    "    prediction_gd = inference(trained_weights_gd, x)\n",
    "    print(f\"Input: [{x[0].item()}, {x[1].item()}], Target: {y.item()}, SGD Prediction: {prediction_sgd}, GD Prediction: {prediction_gd}\")\n",
    "    \n",
    "# Plot error history\n",
    "plot_error_historys(error_history_sgd, error_history_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e6d98",
   "metadata": {},
   "source": [
    "2-2) [10 pts] Modify the initial weight as `[0.68, 0.01, 0.73]` and run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and paste from 2-1 and modify\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9dc38",
   "metadata": {},
   "source": [
    "2-3) Modify the learning rate to `lr = 0.9` and run the code. Then, modify the\n",
    "learning rate to `lr = 100.0` and run the code. use the initial weight of 2-2) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab62f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.9\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 100.0\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a81e52",
   "metadata": {},
   "source": [
    "2-4) Run the code after changing the epoch to `100000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc70afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 100000.0\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f45f47",
   "metadata": {},
   "source": [
    "2-5) Run the code using the parameters below. Now let’s compare batched gradient descent and stochastic gradient descent. Which one shows faster error convergence? Which one shows better minimized error?\n",
    "```python\n",
    "initial_weight = np.array([0.68, 0.01, 0.73])\n",
    "epochs = 1000\n",
    "learning_rate = 0.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e78d85",
   "metadata": {},
   "source": [
    "2-6) Modify the given code and plot loss the with some fixed bias value, $w_3=0, -5, -20, -100, -500$. When changing $w_3$, it is recommended to modify the $x$, $y$ range to $(-5, 1.5 \\times (-w_3))$ as shown in the code provided. What is the difference between our homework and typical optimization problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained weights\n",
    "print(\"Trained weights (SGD):\", trained_weights_sgd)\n",
    "print(\"Trained weights (GD):\", trained_weights_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6330780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w1, w2, w3):\n",
    "    weights = np.array([w1, w2, w3])\n",
    "    z = np.dot(X, weights)\n",
    "    error = Y - sigmoid(z)\n",
    "    return np.mean(error ** 2) \n",
    "\n",
    "w3 = -5\n",
    "plot_surface(lambda w1, w2: loss(w1, w2, w3), (-5, 1.5 * (-w3) + 5), (-5, 1.5 * (-w3) + 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
